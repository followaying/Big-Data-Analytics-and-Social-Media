ggplot(sentiment_df, aes(x = sentiment)) +
geom_bar(aes(fill = sentiment)) +
scale_fill_brewer(palette = "RdGy") +
labs(fill = "Sentiment") +
labs(x = "Sentiment Categories", y = "Number of Tweets") +
ggtitle("Sentiment Analysis of Tweets")
# Assign emotion scores to tweets
emo_scores <- get_nrc_sentiment(clean_text)[ , 1:8]
emo_scores_df <- data.frame(clean_text, emo_scores)
# Calculate proportion of emotions across all tweets
emo_sums <- emo_scores_df[,2:9] %>%
sign() %>%
colSums() %>%
sort(decreasing = TRUE) %>%
data.frame() / nrow(emo_scores_df)
names(emo_sums)[1] <- "Proportion"
# Plot emotion classification
ggplot(emo_sums, aes(x = reorder(rownames(emo_sums), Proportion),
y = Proportion,
fill = rownames(emo_sums))) +
geom_col() +
coord_flip()+
guides(fill = "none") +
scale_fill_brewer(palette = "Dark2") +
labs(x = "Emotion Categories", y = "Proportion of Tweets") +
ggtitle("Emotion Analysis of Tweets")
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
# Convert clean tweet vector into a document corpus (collection of documents)
text_corpus <- VCorpus(VectorSource(clean_text))
# Remove stop words
text_corpus <- text_corpus %>%
tm_map(removeWords, stopwords(kind = "SMART"))
# Transform corpus into a Document Term Matrix and remove 0 entries
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
lda_model <- LDA(dtm, k = 6)
tweet_topics <- tidy(lda_model, matrix = "beta")
library(rsample)
install("rsample")
install.packages("rsample")
library(rsample)
tweet_topics <- tidy(lda_model, matrix = "beta")
top_terms <- tweet_topics %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
api_key <- "AIzaSyC5ETz15tC79ufPNN17EpgQR9fhJYzDuZ8"
client_id <- "1054539266290-kuikj7tp57uln1242jnosv8ekrunm31v.apps.googleusercontent.com"
client_secret <- "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"
# Authenticate to YouTube using the tuber package
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search_Blackpink <- yt_search("blackpink")
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search_Blackpink <- yt_search("blackpink")
# and build an actor network
video_ids_Blackpink <- as.vector(video_search_Blackpink$video_id[1:5])
#with some videos and store their video IDs,
#for which we want to collect comments
# and build an actor network
yt_data_Blackpink <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids_Blackpink,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
yt_actor_network_Blackpink <- yt_data_Blackpink %>% Create("actor")
yt_actor_graph_Blackpink <- Graph(yt_actor_network_Blackpink)
# Transform into an undirected graph
undir_yt_actor_graph_Blackpink <- as.undirected(yt_actor_graph_Blackpink, mode = "collapse")
# Run Louvain algorithm
louvain_yt_actor_Blackpink <- cluster_louvain(undir_yt_actor_graph_Blackpink)
# See sizes of communities
sizes(louvain_yt_actor_Blackpink)
# Visualise the Louvain communities
plot(louvain_yt_actor_Blackpink,
undir_yt_actor_graph_Blackpink,
vertex.label = V(undir_yt_actor_graph_Blackpink)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
write.graph(undir_yt_actor_graph_Blackpink,
file = "undir_yt_actor_graph_Blackpink.graphml",
format = "graphml")
# Run Girvan-Newman (edge-betweenness) algorithm
eb_yt_actor_Blackpink <- cluster_edge_betweenness(undir_yt_actor_graph_Blackpink)
# See sizes of communities
sizes(eb_yt_actor_Blackpink)
# Visualise the edge-betweenness communities
plot(eb_yt_actor_Blackpink,
undir_yt_actor_graph_Blackpink,
vertex.label = V(undir_yt_actor_graph_Blackpink)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
# Visualise the edge-betweenness hierarchy
yt_actor_graph_Blackpink2 <- yt_actor_graph_Blackpink
V(yt_actor_graph_Blackpink2)$name <- V(yt_actor_graph_Blackpink2)$screen_name
undir_yt_actor_graph_Blackpink2 <- as.undirected(yt_actor_graph_Blackpink2, mode = "collapse")
eb_yt_actor_Blackpink2 <- cluster_edge_betweenness(undir_yt_actor_graph_Blackpink2)
is_hierarchical(eb_yt_actor_Blackpink2)
as.dendrogram(eb_yt_actor_Blackpink2)
plot_dendrogram(eb_yt_actor_Blackpink2)
plot_dendrogram(eb_yt_actor_Blackpink2, mode = "dendrogram", xlim = c(1,20))
video_search_Momoland <- yt_search("momoland")
# Choose some videos and store their video IDs,
# for which we want to collect comments
# and build an actor network
video_ids_Momoland <- as.vector(video_search_Momoland$video_id[1:5])
#with some videos and store their video IDs,
#for which we want to collect comments
# and build an actor network
yt_data_Momoland <- Authenticate("youtube", apiKey = api_key) %>%
Collect(videoIDs = video_ids_Momoland,
writeToFile = TRUE,
maxComments = 500,
verbose = TRUE)
yt_actor_network_Momoland <- yt_data_Momoland %>% Create("actor")
yt_actor_graph_Momoland <- Graph(yt_actor_network_Momoland)
# Transform into an undirected graph
undir_yt_actor_graph_Momoland <- as.undirected(yt_actor_graph_Momoland, mode = "collapse")
# Run Louvain algorithm
louvain_yt_actor_Momoland <- cluster_louvain(undir_yt_actor_graph_Momoland)
# See sizes of communities
sizes(louvain_yt_actor_Momoland)
# Visualise the Louvain communities
plot(louvain_yt_actor_Momoland,
undir_yt_actor_graph_Momoland,
vertex.label = V(undir_yt_actor_graph_Momoland)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
write.graph(undir_yt_actor_graph_Momoland,
file = "undir_yt_actor_graph_Momoland.graphml",
format = "graphml")
# Run Girvan-Newman (edge-betweenness) algorithm
eb_yt_actor_Momoland <- cluster_edge_betweenness(undir_yt_actor_graph_Momoland)
# See sizes of communities
sizes(eb_yt_actor_Momoland)
# Visualise the edge-betweenness communities
plot(eb_yt_actor_Momoland,
undir_yt_actor_graph_Momoland,
vertex.label = V(undir_yt_actor_graph_Momoland)$screen_name,
vertex.size = 4,
vertex.label.cex = 0.7)
# Visualise the edge-betweenness hierarchy
yt_actor_graph_Momoland2 <- yt_actor_graph_Momoland
V(yt_actor_graph_Momoland2)$name <- V(yt_actor_graph_Momoland2)$screen_name
undir_yt_actor_graph_Momoland2 <- as.undirected(yt_actor_graph_Momoland2, mode = "collapse")
eb_yt_actor_Momoland2 <- cluster_edge_betweenness(undir_yt_actor_graph_Momoland2)
is_hierarchical(eb_yt_actor_Momoland2)
as.dendrogram(eb_yt_actor_Momoland2)
plot_dendrogram(eb_yt_actor_Momoland2)
plot_dendrogram(eb_yt_actor_Momoland2, mode = "dendrogram", xlim = c(1,20))
# Get songs from Blackpink and their audio features
Blackpink_features <- get_artist_audio_features("Blackpink")
data.frame(colnames(Blackpink_features))
Blackpink_features_subset <- Blackpink_features[ , 9:20]
# Get top 100 songs and their audio features
top100_features <- get_playlist_audio_features("spotify", "4hOKQuZbraPDIfaGbM3lKI")
data.frame(colnames(top100_features))
top100_features_subset <- top100_features[ , 6:17]
top100_features_subset <- top100_features_subset %>% rename(track_id = track.id)
# Add the 'isBlackpink' column (class variable) to each data frame
# to indicate which songs are by Blackpink and which are not
top100_features_subset["isBlackpink"] <- 0
Blackpink_features_subset["isBlackpink"] <- 1
# Remove any songs by Blackpink that appear in the top 100
# and combine the two data frames into one dataset
top100_features_noBlackpink <- anti_join(top100_features_subset,
Blackpink_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noBlackpink, Blackpink_features_subset)
# Format the dataset so that we can give it as input to a model:
# change the 'isBlackpink' column into a factor
# and remove the 'track_id' column
comb_data$isBlackpink <- factor(comb_data$isBlackpink)
comb_data <- select(comb_data, -track_id)
# Randomise the dataset (shuffle the rows)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
# Split the dataset into training and testing sets (80% training, 20% testing)
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
# Train the decision tree model
dt_model <- train(isBlackpink~ ., data = training_set, method = "C5.0")
# Sample a single prediction (can repeat)
prediction_row <- 1 # MUST be smaller than or equal to training set size
if (tibble(predict(dt_model, testing_set[prediction_row, ])) ==
testing_set[prediction_row, 12]){
print("Prediction is correct!")
} else {
("Prediction is wrong")
}
# Analyse the model accuracy with a confusion matrix
confusionMatrix(dt_model, reference = testing_set$isBlackpink)
# Get songs from Momoland and their audio features
Momoland_features <- get_artist_audio_features("Momoland")
data.frame(colnames(Momoland_features))
Momoland_features_subset <- Momoland_features[ , 9:20]
# Get top 100 songs and their audio features
top100_features <- get_playlist_audio_features("spotify", "4hOKQuZbraPDIfaGbM3lKI")
data.frame(colnames(top100_features))
top100_features_subset <- top100_features[ , 6:17]
top100_features_subset <- top100_features_subset %>% rename(track_id = track.id)
# Add the 'isMomoland' column (class variable) to each data frame
# to indicate which songs are by Momoland and which are not
top100_features_subset["isMomoland"] <- 0
Momoland_features_subset["isMomoland"] <- 1
# Remove any songs by Momoland that appear in the top 100
# and combine the two data frames into one dataset
top100_features_noMomoland <- anti_join(top100_features_subset,
Momoland_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noMomoland, Momoland_features_subset)
# Format the dataset so that we can give it as input to a model:
# change the 'isMomoland' column into a factor
# and remove the 'track_id' column
comb_data$isMomoland <- factor(comb_data$isMomoland)
comb_data <- select(comb_data, -track_id)
# Randomise the dataset (shuffle the rows)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
# Split the dataset into training and testing sets (80% training, 20% testing)
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
# Train the decision tree model
dt_model <- train(isMomoland~ ., data = training_set, method = "C5.0")
# Sample a single prediction (can repeat)
prediction_row <- 1 # MUST be smaller than or equal to training set size
if (tibble(predict(dt_model, testing_set[prediction_row, ])) ==
testing_set[prediction_row, 12]){
print("Prediction is correct!")
} else {
("Prediction is wrong")
}
# Analyse the model accuracy with a confusion matrix
confusionMatrix(dt_model, reference = testing_set$isMomoland)
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
twitter_data <- readRDS("D:/GU/Big data/Assignment/Ass1/2023-03-31_001726-TwitterData.rds")
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
text_corpus <- VCorpus(VectorSource(clean_text))
# Remove stop words
text_corpus <- text_corpus %>%
tm_map(removeWords, stopwords(kind = "SMART"))
# Transform corpus into a Document Term Matrix and remove 0 entries
doc_term_matrix <- DocumentTermMatrix(text_corpus)
non_zero_entries = unique(doc_term_matrix$i)
dtm = doc_term_matrix[non_zero_entries,]
# Optional: Remove objects and run garbage collection for faster processing
save(dtm, file = "doc_term_matrix.RData")
rm(list = ls(all.names = TRUE))
gc()
load("doc_term_matrix.RData")
View(dtm)
api_key <- "AIzaSyC5ETz15tC79ufPNN17EpgQR9fhJYzDuZ8"
client_id <- "1054539266290-kuikj7tp57uln1242jnosv8ekrunm31v.apps.googleusercontent.com"
client_secret <- "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"
# Authenticate to YouTube using the tuber package
yt_oauth(app_id = client_id, app_secret = client_secret)
#library-------
library(vosonSML)
library(magrittr)
library(tidyr)
library(tidytext)
library(stopwords)
library(textclean)
library(qdapRegex)
library(tm)
library(SnowballC)
library(Rspotify)
library(spotifyr)
library(igraph)
library(dplyr)
library(knitr)
library(ggplot2)
library(ggridges)
library(httpuv)
library(tuber)
library(syuzhet)
library(C50)
library(caret)
library(e1071)
library(dplyr)
library(topicmodels)
library(slam)
library(Rmpfr)
library(reshape2)
library(topicmodels)
library(askpass)
library(rsample)
api_key <- "AIzaSyC5ETz15tC79ufPNN17EpgQR9fhJYzDuZ8"
client_id <- "1054539266290-kuikj7tp57uln1242jnosv8ekrunm31v.apps.googleusercontent.com"
client_secret <- "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"
# Authenticate to YouTube using the tuber package
yt_oauth(app_id = client_id, app_secret = client_secret)
video_search <- yt_search("#twice OR twice kpop")
yt_oauth(app_id = client_id, app_secret = client_secret)
twitter_data <- readRDS("D:/GU/Big data/Assignment/Ass1/2023-03-31_001726-TwitterData.rds")
clean_text <- twitter_data$tweets$text  %>%
rm_twitter_url() %>%
replace_url() %>%
replace_hash() %>%
replace_tag() %>%
replace_internet_slang() %>%
replace_emoji() %>%
replace_emoticon() %>%
replace_non_ascii() %>%
replace_contraction() %>%
gsub("[[:punct:]]", " ", .) %>%
gsub("[[:digit:]]", " ", .) %>%
gsub("[[:cntrl:]]", " ", .) %>%
gsub("\\s+", " ", .) %>%
tolower()
sentiment_scores <- get_sentiment(clean_text, method = "afinn") %>% sign()
sentiment_df <- data.frame(text = clean_text, sentiment = sentiment_scores)
# Convert sentiment scores to labels: positive, neutral, negative
sentiment_df$sentiment <- factor(sentiment_df$sentiment, levels = c(1, 0, -1),
labels = c("Positive", "Neutral", "Negative"))
View(sentiment_df)
emo_scores <- get_nrc_sentiment(clean_text)[ , 1:8]
emo_scores_df <- data.frame(clean_text, emo_scores)
# Calculate proportion of emotions across all tweets
emo_sums <- emo_scores_df[,2:9] %>%
sign() %>%
colSums() %>%
sort(decreasing = TRUE) %>%
data.frame() / nrow(emo_scores_df)
names(emo_sums)[1] <- "Proportion"
View(emo_sums)
Twice_features <- get_artist_audio_features("Twice")
app_id <- "67d3dd0bcc304eb0968de7be9857a1f5"
app_secret <- "e2f2696b6a5848048efed21ae9aef2d4"
token <- "1"
# Authentication for Rspotify package:
keys <- spotifyOAuth(token, app_id, app_secret)
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
Twice_features <- get_artist_audio_features("Twice")
data.frame(colnames(Twice_features))
Twice_features_subset <- Twice_features[ , 9:20]
# Get top 100 songs and their audio features
top100_features <- get_playlist_audio_features("spotify", "4hOKQuZbraPDIfaGbM3lKI")
data.frame(colnames(top100_features))
top100_features_subset <- top100_features[ , 6:17]
top100_features_subset <- top100_features_subset %>% rename(track_id = track.id)
# Add the 'isTwice' column (class variable) to each data frame
# to indicate which songs are by Twice and which are not
top100_features_subset["isTwice"] <- 0
Twice_features_subset["isTwice"] <- 1
# Remove any songs by Twice that appear in the top 100
# and combine the two data frames into one dataset
top100_features_noTwice <- anti_join(top100_features_subset,
Twice_features_subset,
by = "track_id")
comb_data <- rbind(top100_features_noTwice, Twice_features_subset)
# Format the dataset so that we can give it as input to a model:
# change the 'isTwice' column into a factor
# and remove the 'track_id' column
comb_data$isTwice <- factor(comb_data$isTwice)
comb_data <- select(comb_data, -track_id)
# Randomise the dataset (shuffle the rows)
comb_data <- comb_data[sample(1:nrow(comb_data)), ]
# Split the dataset into training and testing sets (80% training, 20% testing)
split_point <- as.integer(nrow(comb_data)*0.8)
training_set <- comb_data[1:split_point, ]
testing_set <- comb_data[(split_point + 1):nrow(comb_data), ]
# Train the decision tree model
dt_model <- train(isTwice~ ., data = training_set, method = "C5.0")
View(testing_set)
prediction_row <- 8
if (tibble(predict(dt_model, testing_set[prediction_row, ])) ==
testing_set[prediction_row, 12]){
print("Prediction is correct!")
} else {
("Prediction is wrong")
}
# Analyse the model accuracy with a confusion matrix
confusionMatrix(dt_model, reference = testing_set$isTwice)
options(httr_oauth_cache = TRUE)
# Set up authentication variables
app_id <- "67d3dd0bcc304eb0968de7be9857a1f5"
app_secret <- "e2f2696b6a5848048efed21ae9aef2d4"
token <- "1"
keys <- spotifyOAuth(token, app_id, app_secret)
library-------
library(vosonSML)
#library-------
library(vosonSML)
library(magrittr)
library(tidyr)
library(tidytext)
library(stopwords)
library(textclean)
library(qdapRegex)
library(tm)
library(SnowballC)
library(Rspotify)
library(spotifyr)
library(igraph)
library(dplyr)
library(knitr)
library(ggplot2)
library(ggridges)
library(httpuv)
library(tuber)
library(syuzhet)
library(C50)
library(caret)
library(e1071)
library(dplyr)
library(topicmodels)
library(slam)
library(Rmpfr)
library(reshape2)
library(topicmodels)
library(askpass)
library(rsample)
# Authentication for Rspotify package:
keys <- spotifyOAuth(token, app_id, app_secret)
# Get Spotify data on 'Twice'
find_my_artist <- searchArtist("Twice", token = keys)
my_artist <- getArtist("7n2Ycct7Beij7Dj7meI4X0", token = keys)
# Authentication for spotifyr package:
Sys.setenv(SPOTIFY_CLIENT_ID = app_id)
Sys.setenv(SPOTIFY_CLIENT_SECRET = app_secret)
access_token <- get_spotify_access_token()
#get all products own by 'Twice'
Twice <-get_artist_audio_features(artist = "7n2Ycct7Beij7Dj7meI4X0",
include_groups = c("album", "single"),
return_closest_artist = TRUE,
dedupe_albums = TRUE,
market = NULL,
authorization = get_spotify_access_token())
Twice <- Twice[!duplicated(Twice$track_name),]
#calculate the number of active year
earliest_year <- min(Twice$album_release_year)
num_years_active <- (as.integer(format(Sys.Date(), "%Y")) - earliest_year)
# Retrieve album data of artist
albums <- getAlbums("7n2Ycct7Beij7Dj7meI4X0", token = keys)
# Get audio features for 'Twice'
audio_features <- get_artist_audio_features("Twice")
audio_feature <- audio_features[!duplicated(audio_features$track_name, audio_features$album_name), ]
# Plot happiness (valence) scores for each album
ggplot(audio_features, aes(x = valence, y = album_name)) +
geom_density_ridges() +
theme_ridges() +
ggtitle("Prevalent features in Twice Albums",
subtitle = "Based on valence from Spotify's Web API")
View(audio_feature)
# Plot liveness scores for each album
ggplot(audio_features, aes(x = liveness, y = album_name)) +
geom_density_ridges() +
theme_ridges() +
ggtitle("Liveness features in Twice Albums",
subtitle = "Based on valence from Spotify's Web API")
# Plot happiness (valence) scores for each album
ggplot(audio_features, aes(x = valence, y = album_name)) +
geom_density_ridges() +
theme_ridges() +
ggtitle("Prevalent features in Twice Albums",
subtitle = "Based on valence from Spotify's Web API")
